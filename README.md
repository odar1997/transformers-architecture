# transformers-architecture

## StoryTelling 


---

# üß† Transformers: La Rebeli√≥n de los Autobots en la Tierra de la IA

Hace no mucho tiempo, en el vasto universo del procesamiento del lenguaje natural, dos facciones se disputaban el dominio: los **Decepticons**, encarnados por las arquitecturas tradicionales como las **RNN** y **LSTM**, y los **Autobots**, una nueva raza de modelos conocida como **Transformers**.

Durante a√±os, los Decepticons reinaron con fuerza. Las **RNN** recorr√≠an el texto paso a paso, atrapadas en la linealidad del tiempo. Las **LSTM**, m√°s poderosas, intentaban recordar el pasado distante, pero su memoria era limitada, y su poder decay√≥ al enfrentarse a secuencias largas y complejas. Las batallas eran lentas, y el entrenamiento costoso. El trono de la inteligencia artificial temblaba ante la necesidad de algo... m√°s fuerte.

Y entonces, llegaron **ellos**.

## ü§ñ El Ascenso de los Transformers (Autobots)

Liderados por **Optimus Prime**, el modelo de atenci√≥n, los Transformers aterrizaron con una propuesta revolucionaria:

> *"No necesitamos recorrer las secuencias paso a paso. Miraremos TODO al mismo tiempo."*

La **Self-Attention** fue su arma secreta: una forma de enfocarse en las partes m√°s importantes de la secuencia sin perder contexto, sin importar cu√°n lejos estuviera la informaci√≥n relevante. Ya no hab√≠a l√≠mites de memoria, ya no hab√≠a cuellos de botella.

### Las Ventajas de los Autobots

* üî≠ **Atenci√≥n global**: pueden mirar todos los tokens a la vez y decidir qu√© es importante.
* üöÄ **Paralelizaci√≥n**: entrenan mucho m√°s r√°pido que sus enemigos, al no depender del tiempo.
* üß† **Escalabilidad**: pueden crecer hasta convertirse en titanes como BERT, GPT, o T5.

Y as√≠, los Transformers liberaron al NLP del yugo secuencial. Los Decepticons se replegaron, usados solo en tareas peque√±as, obsoletos frente al poder del paralelismo y la atenci√≥n.

---

## üìú Legado

Hoy, los Autobots dominan las tierras del procesamiento de lenguaje natural, visi√≥n por computadora, m√∫sica, bioinform√°tica y m√°s. Cada vez que entrenas un modelo basado en atenci√≥n, est√°s invocando el legado de Optimus Prime y su ej√©rcito.

Y recuerda:

> *"Hasta que todos los tokens sean atendidos."* ‚Äì Optimus Prime 

---

## Tecnical presentation

ÔøΩ
ÔøΩ
 Clase: La Arquitectura Transformer ‚Äì Historia, 
Funcionamiento y Relevancia 
1. Historia y contexto: El nacimiento del Transformer 
Durante muchos a√±os, las tareas de procesamiento de lenguaje natural (NLP) se abordaron 
principalmente con redes neuronales recurrentes (RNN) y, m√°s tarde, con LSTM (Long 
Short-Term Memory). Estos modelos fueron fundamentales en avances como traducci√≥n 
autom√°tica y generaci√≥n de texto, pero ten√≠an limitaciones significativas. Su procesamiento 
secuencial los hac√≠a lentos, y ten√≠an dificultades para capturar relaciones a largo plazo entre 
palabras en una oraci√≥n. 
En 2017, un grupo de investigadores de Google Brain, liderados por Ashish Vaswani, 
propuso una nueva arquitectura en el art√≠culo titulado "Attention Is All You Need". Esta 
publicaci√≥n revolucion√≥ el campo del NLP al introducir el modelo Transformer, una 
arquitectura que eliminaba completamente el uso de recurrencia y que se basaba en 
mecanismos de atenci√≥n para modelar relaciones entre elementos de una secuencia. Esta 
innovaci√≥n dio lugar a un salto de calidad en tareas como traducci√≥n, clasificaci√≥n de texto y 
m√°s, y sent√≥ las bases de modelos avanzados como BERT, GPT, T5, y otros. 
2. Motivaci√≥n: ¬øPor qu√© reemplazar a las RNN y LSTM? 
Las redes recurrentes procesan el texto palabra por palabra, lo que las hace 
inherentemente secuenciales. Esto implica que no pueden aprovechar del todo el 
paralelismo de los procesadores modernos como las GPUs. Adem√°s, a medida que las 
secuencias se hacen m√°s largas, su capacidad para recordar informaci√≥n de palabras 
distantes disminuye, lo cual afecta su comprensi√≥n del contexto. 
El Transformer rompe con esta secuencialidad. Procesa todo el texto al mismo tiempo, 
capturando de forma eficiente las relaciones entre todas las palabras, sin importar cu√°n 
alejadas est√©n unas de otras. Adem√°s, gracias a su dise√±o, puede entrenarse m√°s 
r√°pidamente y escalar a modelos mucho m√°s grandes que sus predecesores. 
3. Visi√≥n general de la arquitectura Transformer 
La arquitectura Transformer est√° dividida en dos grandes bloques: el codificador (encoder) 
y el decodificador (decoder). Cada uno de ellos est√° compuesto por varias capas (por 
ejemplo, 6 capas en el modelo original) que contienen subcomponentes especializados. 
‚óè Encoder: toma como entrada una secuencia de tokens (palabras convertidas en 
vectores) y produce una representaci√≥n contextualizada de cada uno. Cada capa del 
encoder aplica mecanismos de self-attention y redes feed-forward densas. 
‚óè Decoder: recibe los embeddings de salida ya generados (como palabras previas en 
la generaci√≥n de texto) y los embeddings del encoder. As√≠, produce la siguiente 
palabra en una secuencia, utilizando atenci√≥n cruzada (cross-attention) y 
self-attention para generar texto coherente. 
4. Self-Attention: El coraz√≥n del Transformer 
El mecanismo de self-attention permite que cada palabra en una oraci√≥n mire a todas las 
dem√°s para decidir qu√© tan importante es cada una en su contexto. Por ejemplo, en la frase 
"La ni√±a dej√≥ el libro porque estaba cansada", el modelo debe entender que "cansada" se 
refiere a "ni√±a", no a "libro". 
El self-attention se implementa utilizando tres vectores para cada palabra: 
‚óè Query (Q): lo que buscamos. 
‚óè Key (K): lo que ofrecemos. 
‚óè Value (V): la informaci√≥n que compartimos. 
La atenci√≥n se calcula como: 
Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd_k) * V 
Esta f√≥rmula mide la similitud entre queries y keys para determinar cu√°nto "peso" darle a 
cada value. 
5. Multi-Head Attention: Diversidad de perspectivas 
Una sola capa de atenci√≥n puede capturar una relaci√≥n entre palabras, pero a veces son 
necesarias m√∫ltiples perspectivas (por ejemplo, relaciones gramaticales, sem√°nticas, 
sint√°cticas). Por eso, el Transformer usa multi-head attention, que consiste en replicar el 
mecanismo de atenci√≥n varias veces en paralelo con diferentes pesos. Cada cabeza 
aprende una forma distinta de representar las relaciones entre palabras. Luego, se 
combinan todas las salidas en una sola matriz que se pasa a la siguiente capa. 
6. Positional Encoding: Incorporar el orden de las palabras 
A diferencia de las RNN, el Transformer no tiene una estructura secuencial interna, por lo 
que necesita otra forma de entender el orden de las palabras. Para esto, se agregan 
vectores de codificaci√≥n posicional a los embeddings de las palabras. Estos vectores 
usan funciones seno y coseno de diferentes frecuencias para representar posiciones, lo que 
permite al modelo distinguir entre "El perro muerde al hombre" y "El hombre muerde al 
perro". 
7. Feed-Forward Networks y otras capas 
Despu√©s del m√≥dulo de atenci√≥n, cada capa del Transformer incluye una red feed-forward 
completamente conectada, que aplica transformaciones no lineales a los datos de cada 
posici√≥n. Tambi√©n se emplean dos t√©cnicas importantes: 
‚óè Normalization (Layer Normalization): estabiliza el entrenamiento. 
‚óè Residual Connections: conexiones que saltan capas para evitar el problema del 
desvanecimiento del gradiente. 
Cada capa puede entonces ajustar la representaci√≥n de cada token de manera refinada y 
robusta. 
8. El Decoder y su atenci√≥n cruzada 
El decoder funciona de manera similar al encoder, pero con una diferencia clave: incluye 
una capa de cross-attention, que permite que el decoder se enfoque en partes relevantes 
de la salida del encoder mientras genera el texto final. Tambi√©n incorpora m√°scaras para 
que no pueda mirar las palabras futuras durante el entrenamiento (lo que se llama "masked 
self-attention"). 
9. Impacto y aplicaciones del Transformer 
La arquitectura Transformer ha sido adoptada como base para los modelos m√°s potentes y 
√∫tiles de la actualidad: 
‚óè GPT (Generative Pre-trained Transformer): usado para generaci√≥n de texto, 
chatbots, asistentes virtuales. 
‚óè BERT (Bidirectional Encoder Representations from Transformers): dise√±ado 
para comprender texto y realizar tareas como clasificaci√≥n, extracci√≥n de 
informaci√≥n, etc. 
‚óè T5 (Text-to-Text Transfer Transformer): convierte cualquier tarea de NLP en una 
tarea de traducci√≥n entre formatos de texto. 
‚óè Vision Transformers (ViT): aplican Transformers a tareas de visi√≥n por 
computadora, como clasificaci√≥n de im√°genes. 
Tambi√©n se usan en m√∫sica, biolog√≠a (como en la predicci√≥n de estructuras de prote√≠nas), y 
mucho m√°s. 
10. Conclusi√≥n y reflexiones 
La arquitectura Transformer representa un cambio de paradigma en el dise√±o de modelos 
para inteligencia artificial, especialmente en el lenguaje natural. Su capacidad para modelar 
relaciones complejas sin necesidad de procesar secuencias paso a paso ha permitido 
desarrollar sistemas m√°s precisos, r√°pidos y escalables. Su dise√±o modular y elegante ha 
inspirado decenas de variantes y sigue evolucionando con nuevas investigaciones. 
Aprender c√≥mo funciona un Transformer es esencial para comprender c√≥mo trabajan los 
modelos de lenguaje m√°s avanzados de la actualidad, incluyendo aquellos con los que 
interactuamos todos los d√≠as. 
